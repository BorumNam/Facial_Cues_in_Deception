{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7983719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### SHAP input ######\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "police_path = 'F:/FacialExpression/Police/TimeSeriesAnalysisResults'\n",
    "police_output_path = 'F:/FacialExpression/Police/TimeSeriesAnalysisResults/SHAP_input'\n",
    "stim_path = 'F:/FacialExpression/3rdYear_stim/TimeSeriesAnalysisResults'\n",
    "stim_output_path = 'F:/FacialExpression/3rdYear_stim/TimeSeriesAnalysisResults/SHAP_input'\n",
    "mock_path = 'F:/FacialExpression/3rdYear/TimeSeriesAnalysisResults'\n",
    "mock_output_path = 'F:/FacialExpression/3rdYear/TimeSeriesAnalysisResults/SHAP_input'\n",
    "\n",
    "ME_list = ['AU01','AU02','AU04','AU05','AU06','AU07','AU09','AU12','AU14','AU15','AU17','AU20','AU23','AU25','AU26']\n",
    "Anxiety_list = ['AU24','AU28','AU45']\n",
    "EC_control_list = ['AU01','AU02','AU04','AU05','AU06','AU07','AU09','AU10','AU12','AU14','AU15','AU17','AU20','AU23','AU25','AU26','AU28', 'non_AU45', 'Neutral']\n",
    "\n",
    "path = mock_path\n",
    "output_path = mock_output_path\n",
    "\n",
    "ME_row_indices, ME_col_indices = [], []\n",
    "Anxiety_row_indices, Anxiety_col_indices = [], []\n",
    "EC_control_row_indices, EC_control_col_indices = [], []\n",
    "\n",
    "# Collect row and column indices for p-values <= 0.05\n",
    "for pval in sorted(os.listdir(path)):\n",
    "    if pval.split('.')[0].split('_')[-1] == 'pvalue':\n",
    "        if pval.split('.')[0].split('_')[1] == 'ME':\n",
    "            df = pd.read_csv(os.path.join(path, pval))\n",
    "            df = df.to_numpy()\n",
    "            df = df[:, 1:-1]  # Exclude ME Sum\n",
    "            ME_row_indices, ME_col_indices = np.where(df <= 0.05)\n",
    "        elif pval.split('.')[0].split('_')[1] == 'Anxiety':\n",
    "            df = pd.read_csv(os.path.join(path, pval))\n",
    "            df = df.to_numpy()\n",
    "            df = df[:, 1:-1]  # Exclude Blink rate\n",
    "            Anxiety_row_indices, Anxiety_col_indices = np.where(df <= 0.05)\n",
    "        elif pval.split('.')[0].split('_')[1] == 'EC':\n",
    "            df = pd.read_csv(os.path.join(path, pval))\n",
    "            df = df.to_numpy()\n",
    "            df = df[:, 2:-1]  # Exclude Gaze\n",
    "            EC_control_row_indices, EC_control_col_indices = np.where(df <= 0.05)\n",
    "\n",
    "for f in sorted(os.listdir(os.path.join(path, 'MicroExpression'))):\n",
    "    result = []\n",
    "    name = []\n",
    "    \n",
    "    # Process MicroExpression data\n",
    "    df = pd.read_csv(os.path.join(path, 'MicroExpression', f))\n",
    "    df = df.to_numpy()\n",
    "    df = df[:, 1:-1]  # Exclude ME Sum\n",
    "    \n",
    "    for i in range(np.shape(ME_row_indices)[0]):\n",
    "        result.append(df[ME_row_indices[i], ME_col_indices[i]])\n",
    "        frame_num = str(ME_row_indices[i] + 1)\n",
    "        i_name = 'ME_' + ME_list[ME_col_indices[i]] + '_' + frame_num + 'frame'\n",
    "        name.append(i_name)\n",
    "    \n",
    "    # Process Anxiety data\n",
    "    df = pd.read_csv(os.path.join(path, 'Anxiety', f))\n",
    "    df = df.to_numpy()\n",
    "    df = df[:, 1:-1]  # Exclude Blink rate\n",
    "    \n",
    "    for i in range(np.shape(Anxiety_row_indices)[0]):\n",
    "        result.append(df[Anxiety_row_indices[i], Anxiety_col_indices[i]])\n",
    "        frame_num = str(Anxiety_row_indices[i] + 1)\n",
    "        i_name = 'Anxiety_' + Anxiety_list[Anxiety_col_indices[i]] + '_' + frame_num + 'frame'\n",
    "        name.append(i_name)\n",
    "    \n",
    "    # Process EC Control data\n",
    "    df = pd.read_csv(os.path.join(path, 'EC_control', f))\n",
    "    df = df.to_numpy()\n",
    "    df = df[:, 2:-1]  # Exclude Gaze\n",
    "    \n",
    "    for i in range(np.shape(EC_control_row_indices)[0]):\n",
    "        result.append(df[EC_control_row_indices[i], EC_control_col_indices[i]])\n",
    "        frame_num = str(EC_control_row_indices[i] + 1)\n",
    "        i_name = 'EC_control_' + EC_control_list[EC_control_col_indices[i]] + '_' + frame_num + 'frame'\n",
    "        name.append(i_name)\n",
    "    \n",
    "    result = np.array(result)\n",
    "    result_df = pd.DataFrame(result, index=name)\n",
    "    \n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    result_df.to_csv(os.path.join(output_path, f.split('.')[0] + '.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a485adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculating SHAP value ####\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "police_path = 'F:/FacialExpression/Police/TimeSeriesAnalysisResults/SHAP_input'\n",
    "stim_path = 'F:/FacialExpression/3rdYear_stim/TimeSeriesAnalysisResults/SHAP_input'\n",
    "mock_path = 'F:/FacialExpression/3rdYear/TimeSeriesAnalysisResults/SHAP_input'\n",
    "police_output_path = 'F:/FacialExpression/Police/TimeSeriesAnalysisResults/SHAP_Results'\n",
    "stim_output_path = 'F:/FacialExpression/3rdYear_stim/TimeSeriesAnalysisResults/SHAP_Results'\n",
    "mock_output_path = 'F:/FacialExpression/3rdYear/TimeSeriesAnalysisResults/SHAP_Results'\n",
    "\n",
    "path = stim_path\n",
    "output_path = stim_output_path\n",
    "DB_type = 'stim'\n",
    "\n",
    "if DB_type == 'police':\n",
    "    feat_num = 49\n",
    "    sub_num = 74\n",
    "elif DB_type == 'stim':\n",
    "    feat_num = 25\n",
    "    sub_num = 72\n",
    "elif DB_type == 'mock':\n",
    "    feat_num = 738\n",
    "    sub_num = 72\n",
    "    \n",
    "# Data preprocessing (Setting X and y)\n",
    "# Here, you should load the data and perform necessary preprocessing to set X and y.\n",
    "# X: Input data of shape (n_samples, n_features)\n",
    "# y: Binary classification output data of shape (n_samples,) with labels T or F\n",
    "\n",
    "X = np.zeros((len(os.listdir(path)), feat_num))\n",
    "y = np.zeros((len(os.listdir(path))))\n",
    "\n",
    "feature_names = []\n",
    "i = 0\n",
    "for f in sorted(os.listdir(path)):\n",
    "    df = pd.read_csv(os.path.join(path, f))\n",
    "    df = df.to_numpy()\n",
    "    for feat in range(feat_num):\n",
    "        feature_names.append(df[feat, 0])\n",
    "    df = df[:,1:]\n",
    "    label = f.split('.')[0].split('_')[-1]\n",
    "    if label == 'T':\n",
    "        label = 0\n",
    "    elif label == 'F':\n",
    "        label = 1\n",
    "    X[i,:] = df[:,0]\n",
    "    y[i] = label\n",
    "    i += 1\n",
    "\n",
    "# Creating Subject labels\n",
    "subject_labels = np.repeat(range(sub_num), 3)\n",
    "\n",
    "# Leave-One-Subject-Out cross-validation setup\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# XGBoost model configuration\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "\n",
    "# List to store SHAP values\n",
    "shap_values_list = []\n",
    "accuracies = []  # List to store accuracies\n",
    "all_true_labels = []  # Store all true labels from all iterations\n",
    "all_pred_labels = []  # Store all predicted labels from all iterations\n",
    "\n",
    "# Leave-One-Subject-Out cross-validation\n",
    "for train_idx, test_idx in logo.split(X, y, groups=subject_labels):\n",
    "    \n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Creating an XGBoost model and setting class weights\n",
    "    model = xgb.XGBClassifier(n_estimators=100, max_depth=6,\n",
    "        scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    # Model training\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Calculating SHAP values\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "    shap_values_list.append(shap_values)\n",
    "\n",
    "    # Model evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    all_true_labels.extend(y_test)\n",
    "    all_pred_labels.extend(y_pred_binary)\n",
    "    \n",
    "    # Calculating accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculating SHAP values for all subjects\n",
    "shap_values_ = np.reshape(shap_values_list, (-1, feat_num))\n",
    "\n",
    "# Calculating mean accuracy\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "\n",
    "# Calculating confusion matrix for the entire dataset\n",
    "conf_matrix = confusion_matrix(all_true_labels, all_pred_labels)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "\n",
    "# Calculating F1 Score\n",
    "f1 = f1_score(all_true_labels, all_pred_labels)\n",
    "\n",
    "# Calculating Precision\n",
    "precision = precision_score(all_true_labels, all_pred_labels)\n",
    "\n",
    "# Calculating Recall\n",
    "recall = recall_score(all_true_labels, all_pred_labels)\n",
    "\n",
    "# Calculating Sensitivity\n",
    "sensitivity = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 0])\n",
    "\n",
    "# Calculating Specificity\n",
    "specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.2f}\")\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "\n",
    "# Generating SHAP summary plot\n",
    "shap.summary_plot(shap_values_, X, feature_names=feature_names)\n",
    "\n",
    "# Saving the plot as an image file\n",
    "if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "plt.savefig(os.path.join(output_path, DB_type + '_SHAP_summary_plot.png'), dpi=300, bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
